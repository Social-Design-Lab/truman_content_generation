{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate artificial posts and images to go with them. Results will be in `output > csv_files > posts.csv` and `output > post_pictures`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model name\n",
    "\n",
    "Make sure that the `model_name` parameter is the same as what you downloaded in step 2. For example, if you downloaded the 1.3B model, make sure `model_name=\"EleutherAI/gpt-neo-1.3B\"` in the second line of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from happytransformer import HappyGeneration, GENSettings\n",
    "gpt_neo = HappyGeneration(model_type=\"GPT-NEO\", model_name=\"EleutherAI/gpt-neo-1.3B\", load_path=\"internal/model_gpt_neo/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Pexels API Key\n",
    "\n",
    "Go to https://www.pexels.com/api/, and select \"Get Started.\" Create an account. Once you do, you will receive an API Key, which consists of  about 56 numbers and letters. Please copy this and paste it (surrounded by quotes) into the API_KEY variable below. It should read `API_KEY = '123...890'` where `123...890` is your API key. The Pexels API key offers 200 requests per hour and 20,000 requests per month for free as of August 2022. **This means that this program may only generate a maximum of 200 posts per hour unless the multiple API keys are used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '563492ad6f91700001000001c12998557b2d40488b7bc8d328bccd80'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic variables\n",
    "\n",
    "You will likely only need to adjust these variables to get the results you want. If you are getting poor results, or want to change something specific, see the 'Advanced variables' below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Reddit\n",
    "subreddit = 'Science'  # the name of the subreddit you wish to replicate\n",
    "listing = ''  # controversial, best, hot, new, random, rising, top; leave as listing = '' for default\n",
    "\n",
    "# Generating content\n",
    "num_results = 5  # The number of posts you wish to generate.\n",
    "                 # It may be best to start small with about 5, and then adjust any variables if necessary. \n",
    "                 # As noted in the API section above, this program can only generate 200 posts per hour, \n",
    "                 # so this variable can be set to a maximum of 200. \n",
    "\n",
    "# Setting a range of hours (for example, if posts should have a time from -24:59 to 60:59, enter -20 and -60)\n",
    "smallest_hour = -20\n",
    "largest_hour = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Reddit\n",
    "limit = 10 # number of posts to retrieve from the request; must be >= number_of_inputs\n",
    "\n",
    "# Prompting the model to generate titles\n",
    "number_of_inputs = 10   # the number of inputs to prompt the model with. A good range for this is 4-10. \n",
    "                        # The model will take these inputs (e.g. post titles), and generate something similar.\n",
    "                        # The larger the number, the longer it may take to generate results.\n",
    "                        # Unfortunately, the ideal number is usually dependent on the length and format of the prompts.\n",
    "\n",
    "# Getting image search words\n",
    "num_search_words = 2    # include this many words in each image search. Setting this to 1 may result in \n",
    "                        # empty search results and a random image being placed instead.\n",
    "                        # A number that is too large may also not generate any results. \n",
    "                        # 2-3 is probably best.\n",
    "\n",
    "# Getting the images from Pexels\n",
    "photos_per_request = 2  # the higher the photos_per_request, the more variation there will be in images, \n",
    "                        # but the less relevance the images might have (max 80)\n",
    "image_size = 'large'  # 'original', 'large2x', 'large', 'medium', 'small', 'portrait', 'landscape', or 'tiny' \n",
    "\n",
    "default_query = subreddit  # a search term that is entered when no images could be found using the keyword(s) generated by the algorithm.\n",
    "# change this to any string. For example, default_query = 'science'\n",
    "# NOTE: The default query is here in case no results are found in the image search with the search keyword(s).\n",
    "# It may work well to enter a single word that is similar to the topics of other posts/images.\n",
    "# If there is no such theme, a good default may be 'nature' or 'food' or 'people', for example. \n",
    "# Any post that requires this 'placeholder' image will have 'PLACEHOLDER' at the end of its filename.\n",
    "# If the titles are very short, but there is a theme among them such as 'food', make 'food' the \n",
    "# default_query and increase the photos_per_request to a high number such as 50-80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the code\n",
    "\n",
    "Run the program by selecting *Kernel > Restart Kernel and Run All Cells...* in the menu bar at the top left of the screen. Results will be in `output > csv_files > posts.csv` and `output > post_pictures`. You will see the words, \"Posts generated!!\" at the bottom of the notebook when the code is finished running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordfreq import zipf_frequency\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have the model generate text given examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get text from source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts\n",
    "\n",
    "# subreddit = 'Science'  # the name of the subreddit you wish to replicate\n",
    "# listing = ''  # controversial, best, hot, new, random, rising, top; leave as listing = '' for default\n",
    "# limit = 10 # number of posts to retrieve from the request; must be >= number_of_inputs\n",
    "\n",
    "url = 'https://www.reddit.com/r/' + subreddit + '/' + listing + '.json?limit=' + str(limit)\n",
    "header = {'User-agent': 'Useragentname'}\n",
    "res = requests.get(url,headers=header)\n",
    "posts = res.json()['data']['children']\n",
    "#print(\"Connection status: \", res.status_code)\n",
    "#print(len(posts), \"posts fetched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect desired post attributes\n",
    "titles = []\n",
    "content = []\n",
    "for idx, x in enumerate(posts):\n",
    "    titles.append(posts[idx]['data']['title'])\n",
    "    content.append(posts[idx]['data']['selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number_of_inputs = 10   # the number of inputs to prompt the model with. A good range for this is 4-10. \n",
    "#                         # The larger the number, the longer it may take to generate results.\n",
    "#                         # The model will take these inputs (e.g. post titles), and generate something similar.\n",
    "#                         # Unfortunately, the ideal number is usually dependent on the length and format of the prompts.\n",
    "\n",
    "input_str = \"\"\n",
    "my_titles = titles[1:number_of_inputs]\n",
    "for idx, x in enumerate(my_titles):\n",
    "    input_str += \"Prompt: \" + my_titles[idx] + \"\\n###\\n\"\n",
    "input_str += \"Prompt:\"\n",
    "#print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate titles\n",
    "\n",
    "args = GENSettings(max_length=50, no_repeat_ngram_size=2, do_sample=True, early_stopping=False, top_k=50, temperature=0.7)\n",
    "\n",
    "# num_results = 5\n",
    "text_results = []\n",
    "for i in tqdm(range(num_results)):\n",
    "    result = gpt_neo.generate_text(input_str, args=args)\n",
    "    text_results.append(result.text.partition('\\n')[0]) # take only the first line of each result\n",
    "# for i in range(num_results):\n",
    "#     print(text_results[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get keyword(s) for image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stop words and punctuation for better image searches\n",
    "\n",
    "# First, if there is at least one end punctuation (.!?) in the sentence, delete everything after the last one to avoid incomplete sentences.\n",
    "# But check for abbreviations and edge cases\n",
    "for idx, x in enumerate(text_results):\n",
    "    temp_str = text_results[idx]\n",
    "    while True:\n",
    "        # if there are no ., !, or ? symbols in temp_sentence; or if a . is at index -1, 0, or 1; or if the original string ends with a ., !, or ?\n",
    "        if max(temp_str.rfind(i) for i in \".!?\") == -1 or temp_str.rfind('.') - 2 < 0 or max(text_results[idx].rfind(i) for i in \".!?\") == len(text_results[idx]) - 1:\n",
    "            # return the original string\n",
    "            temp_str = text_results[idx]\n",
    "            break\n",
    "        # if the period is for an abbreviation\n",
    "        if (temp_str[temp_str.rfind('.') - 2] == '.') or (temp_str[temp_str.rfind('.') - 2] == ' ' and temp_str[temp_str.rfind('.') - 1] != 'I'):\n",
    "            # trim the string back until a space character is reached\n",
    "            my_bool = True\n",
    "            while my_bool:\n",
    "                temp_str = temp_str[0:len(temp_str) - 1] \n",
    "                if temp_str[len(temp_str) - 1] == ' ':\n",
    "                    my_bool = False\n",
    "        else:\n",
    "            # the string has been sufficiently trimmed. Return the trimmed string\n",
    "            temp_str = temp_str[0:temp_str.rfind('.') + 1]\n",
    "            break\n",
    "    text_results[idx] = temp_str\n",
    "\n",
    "text_res_tokens = []\n",
    "text_res_tokens_punct = []\n",
    "text_results_punct = text_results.copy()\n",
    "\n",
    "for idx, x in enumerate(text_results):    \n",
    "    # remove punctuation\n",
    "    text_results[idx] = re.sub(r'[^\\w\\s]','',text_results[idx])\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text_results[idx])\n",
    "    word_tokens_punct = word_tokenize(text_results_punct[idx])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence_punct = [w for w in word_tokens_punct if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    filtered_sentence_punct = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w.lower())\n",
    "    for w in word_tokens_punct:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence_punct.append(w.lower())\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)\n",
    "    text_res_tokens.append(filtered_sentence)\n",
    "    #print(word_tokens_punct)\n",
    "    #print(filtered_sentence_punct)\n",
    "    text_res_tokens_punct.append(filtered_sentence_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the least commonly used word for an image search\n",
    "num_words_for_search = 5    # take the top n least common words from each\n",
    "language = 'en'\n",
    "\n",
    "pos_search_words = []\n",
    "for i, x in enumerate(text_res_tokens):\n",
    "    search_words = {}\n",
    "    for j, y in enumerate(text_res_tokens[i]):\n",
    "        search_words[str(text_res_tokens[i][j])] = zipf_frequency(text_res_tokens[i][j], language)\n",
    "    search_words = {k: v for k, v in sorted(search_words.items(), key=lambda item: item[1])[:num_words_for_search]}\n",
    "    # print(search_words)\n",
    "    temp_word_list = []\n",
    "    pos_search_words.append(list(search_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk nouns\n",
    "nltk_search_words = []\n",
    "for idx, x in enumerate(text_results_punct):\n",
    "    lines = text_results_punct[idx]\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(lines)\n",
    "    nouns = [word.lower() for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos) and len(word) > 1] \n",
    "    # print(nouns)\n",
    "    nltk_search_words.append(nouns)\n",
    "# combine these search_words with those from current pos_search_words list above\n",
    "for i, x in enumerate(pos_search_words):\n",
    "    for j, y in enumerate(nltk_search_words[i]):\n",
    "        if nltk_search_words[i][j] not in pos_search_words[i]:\n",
    "            pos_search_words[i].append(nltk_search_words[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classification model\n",
    "classifier = pipeline(\"zero-shot-classification\", 'internal/model_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting image search words\n",
    "# num_search_words = 2    # include this many words in each image search. Setting this to 1 may result in \n",
    "#                         # empty search results and a random image being placed instead.\n",
    "#                         # A number that is too large may also not generate any results. \n",
    "#                         # 2-3 is probably best.\n",
    "\n",
    "# For \"dissallowed_words\", be sure to list words that are very common among responses\n",
    "# For example, if the topics involve scientific research, include words like 'researchers' or 'study' and 'studies'. \n",
    "# Check which keywords result from the algorithm, and if there are many repeats that might clutter an image search, \n",
    "# be sure to include them below\n",
    "disallowed_words = ['question', 'phrase', 'questions', 'phrases', 'sentence', 'sentences', 'article', 'articles',\n",
    "                              'paragraph', 'paragraphs', 'why', 'how', 'did', 'what', 'who', 'where', 'when', 'do',\n",
    "                              'researchers', 'research', 'study']\n",
    "\n",
    "# initialize var\n",
    "final_search_words = []\n",
    "\n",
    "# function for filtering everything but nouns\n",
    "is_noun = lambda pos: (pos[:2] == 'NN' or pos[:2] == 'NNS' or pos[:2] == 'NNP' or pos[:2] == 'NNSP')\n",
    "\n",
    "# get keyword\n",
    "for i, x in enumerate(text_results):\n",
    "    # exclude eveything except nouns (including plural and proper); remove disallowed words\n",
    "    nltk_search_words[i] = [word for (word, pos) in nltk.pos_tag(nltk_search_words[i]) if is_noun(pos) and word not in disallowed_words]\n",
    "    pos_search_words[i] = [word for (word, pos) in nltk.pos_tag(pos_search_words[i]) if is_noun(pos) and word not in disallowed_words]  \n",
    "    \n",
    "    \n",
    "    # words that show up the most frequently and more than once in the nltk nouns get priority\n",
    "\n",
    "    # sort on basis of frequency of elements\n",
    "    temp_nltk_s_w = [item for items, c in Counter(nltk_search_words[i]).most_common() for item in [items] * c]\n",
    "    # remove unique values\n",
    "    for index in range(len(temp_nltk_s_w) - 1, -1, -1):\n",
    "        if temp_nltk_s_w.count(temp_nltk_s_w[index]) == 1:\n",
    "            del temp_nltk_s_w[index]\n",
    "    # temp_nltk_s_w now contains only words that occur more than once in nltk_search_words, sorted \n",
    "    # by those with the most duplicates first.\n",
    "    # temp_nltk_s_w does not contain any unique words. For example: ['word1', 'word1', 'word1', 'word2', word2']\n",
    "    # now, remove duplicates... \n",
    "    temp_nltk_s_w = list(dict.fromkeys(temp_nltk_s_w))\n",
    "    # ...and append this list to the final_search_words (first truncate list if there are too many)\n",
    "    for i in range(0, len(temp_nltk_s_w) - num_search_words):\n",
    "        temp_nltk_s_w.pop()\n",
    "    final_search_words.append(temp_nltk_s_w)\n",
    "    \n",
    "    \n",
    "    # if the list of search words is not completely filled, use classification to find which words in the sentence have the best fit\n",
    "    if len(final_search_words[i]) < num_search_words:\n",
    "        cl_res = classifier(x, pos_search_words[i], multi_label=True)['labels']\n",
    "        fkw_index = 0\n",
    "        while len(final_search_words[i]) < num_search_words and fkw_index < len(pos_search_words[i]):\n",
    "            while cl_res[fkw_index] in temp_nltk_s_w:\n",
    "                fkw_index += 1\n",
    "                if fkw_index >= len(pos_search_words[i]):\n",
    "                    break\n",
    "            final_search_words[i].append(cl_res[fkw_index])\n",
    "        # for reference: classifier(sequence, candidate_labels, multi_label=True)\n",
    "    #print(\"iteration\", i, \":\", final_search_words)\n",
    "    \n",
    "# print(final_search_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get picture and save to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pexels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the higher the photos_per_request, the more variation there will be in images, but the less relevance the images might have (max 80)\n",
    "# photos_per_request = 2\n",
    "# image_size = 'large'  # 'original', 'large2x', 'large', 'medium', 'small', 'portrait', 'landscape', or 'tiny'\n",
    "\n",
    "# API_KEY = '563492ad6f91700001000001c12998557b2d40488b7bc8d328bccd80'\n",
    "# # The API key is relatively easy to get by creating an account at https://www.pexels.com/api/\n",
    "# # It offers 200 requests per hour and 20,000 requests per month for free. \n",
    "\n",
    "# default_query = 'science'\n",
    "# # NOTE: The default query is here in case no results are found in the image search with the keyword(s) found above.\n",
    "# # It may work well to enter a single word that is similar to the topics of other posts/images.\n",
    "# # If there is no such theme, a good default may be 'nature' or 'food' or 'people', for example. \n",
    "# # Any post that requires this 'placeholder' image will have 'PLACEHOLDER' at the end of its filename.\n",
    "# # If the titles are very short, but there is a theme among them such as 'food', make 'food' the \n",
    "# # default_query and increase the photos_per_request to a high number such as 50-80\n",
    "\n",
    "\n",
    "# # range of hours (for example, if posts should have a time from -24:59 to 60:59, enter -20 and -60)\n",
    "# smallest_hour = -20\n",
    "# largest_hour = 60\n",
    "\n",
    "\n",
    "# for csv file\n",
    "post_data = {'id': [],\n",
    "            'body': [],\n",
    "            'picture': [],\n",
    "            'actor': [],\n",
    "            'time': [],\n",
    "            'class': [],\n",
    "            'experiment_group': []\n",
    "}\n",
    "# get list of actors\n",
    "actor_names = list(pd.read_csv('output/csv_files/actors.csv')['username'])\n",
    "\n",
    "for i, x in enumerate(final_search_words):\n",
    "    image_found = 1\n",
    "    # construct search query\n",
    "    my_query = final_search_words[i][0]\n",
    "    for j in range(1, len(final_search_words[i])):\n",
    "        my_query += \"%20\" + final_search_words[i][j]\n",
    "    print(text_results_punct[i])\n",
    "    response = requests.get(f\"https://api.pexels.com/v1/search?query=\"+my_query+\"&per_page=\"+str(photos_per_request)+\", allow_redirects=True\", headers={'Authorization': API_KEY})\n",
    "    if len(response.json()['photos']) == 0: # if nothing found in original query, fill it in with a picture of nature\n",
    "        response = requests.get(f\"https://api.pexels.com/v1/search?query=\"+default_query+\"&per_page=\"+str(photos_per_request)+\", allow_redirects=True\", headers={'Authorization': API_KEY})\n",
    "        image_found = 0\n",
    "    this_image = random.choice(response.json()['photos'])\n",
    "    print(this_image['url'])\n",
    "    # download image\n",
    "    image_data = requests.get(str(this_image['src'][str(image_size)])).content\n",
    "    image_name = 'post_img_' + str(i) + '.png'\n",
    "    if not image_found: image_name = 'post_img_' + str(i) + '_PLACEHOLDER' + '.png'\n",
    "    image_path = 'output/post_pictures/' + image_name\n",
    "    with open(image_path, 'wb') as handler:\n",
    "        handler.write(image_data)\n",
    "    \n",
    "    # make into csv\n",
    "    post_data['id'].append(i)\n",
    "    post_data['body'].append(text_results_punct[i])\n",
    "    post_data['picture'].append(image_name)\n",
    "    post_data['actor'].append(random.choice(actor_names))\n",
    "    post_data['time'].append(str(random.randint(smallest_hour, largest_hour)).zfill(2) + \":\" + str(random.randint(0,59)).zfill(2))\n",
    "    post_data['class'].append(random.choice(['normal', 'cohort']))\n",
    "    post_data['experiment_group'].append(random.choice(['var1', 'var2', 'var3', 'var4']))\n",
    "\n",
    "posts_df = pd.DataFrame(post_data)\n",
    "posts_df.to_csv('output/csv_files/posts.csv', index=False)\n",
    "\n",
    "print(\"\\n\\n\\nPosts generated!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
