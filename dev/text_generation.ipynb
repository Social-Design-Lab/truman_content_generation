{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes about this notebook\n",
    "\n",
    "This notebook was written by me, Ryan Schmid, the one who first began this project. This is where I began experimenting with text generation and pairing that text with images. It resembles a very rough version of the `4_generate_posts.ipynb` notebook and does not have as recent code as what is in that document. However, it has some leftover code from when I was experimenting with various techniques, such as other image and keyword-finding APIs. I am including this in the project as a reference and possible starting point if a future developer wishes to expand upon any of these ideas. \n",
    "\n",
    "**Note: a few file references may need editing**\n",
    "\n",
    "A few thoughts about expanding the project:\n",
    "1. Keyword-finding methods\n",
    "    1. Some APIs/libraries were able to pull out noun phrases from sentences. I had limited success using these, but it could be one approach to consider. \n",
    "1. Image searching\n",
    "    1. The Pexels library consists of stock photos, and many of them look like stock photos. So if you'd like photos to look more authentic, see what Instagram and Facebook might offer for APIs. \n",
    "    1. A Google Image API might have the widest range and could produce images that fit nicely with the text. It also has a very flexible search feature. I tried using it for this reason, but unfortunately, the API I used came back with limited and often very unrelated images after I limited the images to only those that could be used commercially. \n",
    "1. Overall approach to generating posts\n",
    "    1. Starting with finding an image first and then generating text from that might actually be more promising than what I did (the exact opposite). There are numerous examples and tutorials online for generating captions to images using computer vision libraries. At least finding the keywords for the subject(s) of an image and generating text from them would mean that images will likely relate much more closely to the text. Also, finding a way to prompt the GPT-Neo model with keywords and a textual example from a post could be successful. For example, you could get examples of reddit posts or pass in a few examples from an existing Truman project's posts.csv file and have it build a sentence similar to those using the keywords found from the image. \n",
    "1. Prompting GPT-Neo\n",
    "    1. As you can see from the printed result below, the format for the GPT-Neo prompt is:\n",
    "    ```\n",
    "    Prompt: ...\n",
    "    ###\n",
    "    Prompt: ...\n",
    "    ###\n",
    "    ...\n",
    "    ###\n",
    "    Prompt:\n",
    "    ```\n",
    "        This was very useful, and I got the idea from a source online that was able to prompt the model in a similar way to (not the best example, but you get the idea):\n",
    "    ```\n",
    "    Word: person\n",
    "    Sentence: The person went for a walk.\n",
    "    ###\n",
    "    Word: car\n",
    "    Sentence: The car drove 10 miles.\n",
    "    ###\n",
    "    Word: airplane\n",
    "    Sentence:\n",
    "    ```\n",
    "        ... and it would generate something like `The airplane flew to the airport.`. Having a few examples of this \"prompt and response\" type input for the model can be a fairly powerful tool in having it generate something similar to what you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginnings for this project were sourced from https://medium.com/mlearning-ai/text-generation-using-gpt-neo-41877ef586c7\n",
    "and https://www.vennify.ai/gpt-neo-made-easy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a terminal, `pip install happytransformer` or `pip install --upgrade --force-reinstall happytransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import HappyGeneration, GENSettings, GENTrainArgs\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordfreq import zipf_frequency\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: The model used here is the GPT Neo, which may use up to 2.7 billion parameters (~10 GB). This is a free, open-source model built by Open AI. Open AI's largest model, the GPT-3, uses 175 billion parameters and is trained on about 45 TB of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this part takes a while and may download ~5GB of data with the 1.3B model or ~10GB with the 2.7B model\n",
    "gpt_neo = HappyGeneration(model_type=\"GPT-NEO\", model_name=\"EleutherAI/gpt-neo-1.3B\") \n",
    "# to use an different-sized model, replace the model_name with\n",
    "# \"EleutherAI/gpt-neo-125M\" or \"EleutherAI/gpt-neo-1.3B\" or \"EleutherAI/gpt-neo-2.7B\"\n",
    "\n",
    "gpt_neo.save(\"internal/model_gpt_neo/\")\n",
    "\n",
    "print (gpt_neo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a previously downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_neo = HappyGeneration(model_type=\"GPT-NEO\", model_name=\"EleutherAI/gpt-neo-1.3B\", load_path=\"internal/model_gpt_neo/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with a string as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For documentation, see https://happytransformer.com/text-generation/settings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = GENSettings(max_length=100, no_repeat_ngram_size=2, do_sample=True, early_stopping=False, top_k=50, temperature=0.7)\n",
    "\n",
    "result = gpt_neo.generate_text(\"write a Reddit post about food.\", args=args)\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have the model generate text given examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts\n",
    "subreddit = 'Science'\n",
    "limit = 50\n",
    "listing = '' # controversial, best, hot, new, random, rising, top\n",
    "\n",
    "url = 'https://www.reddit.com/r/' + subreddit + '/' + listing + '.json?limit=' + str(limit)\n",
    "header = {'User-agent': 'Useragentname'}\n",
    "res = requests.get(url,headers=header)\n",
    "print(\"Connection status: \", res.status_code)\n",
    "posts = res.json()['data']['children']\n",
    "print(len(posts), \"posts fetched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect desired post attributes\n",
    "titles = []\n",
    "content = []\n",
    "for idx, x in enumerate(posts):\n",
    "    titles.append(posts[idx]['data']['title'])\n",
    "    content.append(posts[idx]['data']['selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_inputs = 10\n",
    "\n",
    "input_str = \"\"\n",
    "my_titles = titles[1:number_of_inputs]\n",
    "for idx, x in enumerate(my_titles):\n",
    "    input_str += \"Prompt: \" + my_titles[idx] + \"\\n###\\n\"\n",
    "input_str += \"Prompt:\"\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = GENSettings(max_length=50, no_repeat_ngram_size=2, do_sample=True, early_stopping=False, top_k=50, temperature=0.7)\n",
    "\n",
    "num_results = 5\n",
    "text_results = []\n",
    "for i in tqdm(range(num_results)):\n",
    "    result = gpt_neo.generate_text(input_str, args=args)\n",
    "    text_results.append(result.text.partition('\\n')[0]) # take only the first line of each result\n",
    "for i in range(num_results):\n",
    "    print(text_results[i] + \"\\n\")\n",
    "\n",
    "# remove partial sentences at the end\n",
    "# downside is time\n",
    "# gpt neo api\n",
    "# gpt 3 api\n",
    "# run with 2.7B model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the generated text, and find pictures to go along with each instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install wordfreq`\n",
    "`pip install textblob`\n",
    "`pip install simplejson`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stop words and punctuation for better image searches\n",
    "text_res_tokens = []\n",
    "text_res_tokens_punct = []\n",
    "text_results_punct = text_results.copy()\n",
    "for idx, x in enumerate(text_results):\n",
    "    # remove punctuation\n",
    "    text_results[idx] = re.sub(r'[^\\w\\s]','',text_results[idx])\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text_results[idx])\n",
    "    word_tokens_punct = word_tokenize(text_results_punct[idx])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence_punct = [w for w in word_tokens_punct if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    filtered_sentence_punct = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w.lower())\n",
    "    for w in word_tokens_punct:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence_punct.append(w.lower())\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)\n",
    "    text_res_tokens.append(filtered_sentence)\n",
    "    #print(word_tokens_punct)\n",
    "    #print(filtered_sentence_punct)\n",
    "    text_res_tokens_punct.append(filtered_sentence_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the least commonly used word for an image search\n",
    "num_words_for_search = 5    # take the top n least common words from each\n",
    "language = 'en'\n",
    "\n",
    "pos_search_words = []\n",
    "for i, x in enumerate(text_res_tokens):\n",
    "    search_words = {}\n",
    "    for j, y in enumerate(text_res_tokens[i]):\n",
    "        search_words[str(text_res_tokens[i][j])] = zipf_frequency(text_res_tokens[i][j], language)\n",
    "    #print(search_words)\n",
    "    search_words = {k: v for k, v in sorted(search_words.items(), key=lambda item: item[1])[:num_words_for_search]}\n",
    "    print(search_words)\n",
    "    temp_word_list = []\n",
    "    pos_search_words.append(list(search_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk nouns\n",
    "nltk_search_words = []\n",
    "for idx, x in enumerate(text_results_punct):\n",
    "    lines = text_results_punct[idx]\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(lines)\n",
    "    nouns = [word.lower() for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos) and len(word) > 1] \n",
    "    print(nouns)\n",
    "    nltk_search_words.append(nouns)\n",
    "# combine these search_words with those from current pos_search_words list above\n",
    "for i, x in enumerate(pos_search_words):\n",
    "    for j, y in enumerate(nltk_search_words[i]):\n",
    "        if nltk_search_words[i][j] not in pos_search_words[i]:\n",
    "            pos_search_words[i].append(nltk_search_words[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for overlap in textblob nouns and nltk nouns?\n",
    "# get verbs too? adjectives?\n",
    "# search for first (max 10) words in google image search api?\n",
    "\n",
    "# put sentiment in search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. least commonly used word before first punctuation (except ' and \")\n",
    "# 2. repeated in nltk nouns\n",
    "# 3. in both nltk nouns and least common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier.save_pretrained('internal/model_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", 'internal/model_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"A study shows that the world could be facing a ’climate-change-induced, ice-free‘ year around the end of this century. One theory suggests that if the Earth keeps warming at the current rate, the ice sheets in Greenland\"\n",
    "candidate_labels = ['climatechangeinduced',\n",
    "  'icefree',\n",
    "  'greenland',\n",
    "  'warming',\n",
    "  'sheets',\n",
    "  'study',\n",
    "  'world',\n",
    "  'year',\n",
    "  'end',\n",
    "  'century',\n",
    "  'theory',\n",
    "  'Earth',\n",
    "  'rate',\n",
    "  'ice',\n",
    "  'Greenland']\n",
    "\n",
    "cl_res = classifier(sequence, candidate_labels, multi_label=True)\n",
    "#cl_res['labels'][0]\n",
    "cl_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_search_words = 2  # include this many words in each search. Setting this to 1 may result in \n",
    "    # empty search results and a random image being placed instead\n",
    "\n",
    "# For \"dissallowed_words\", be sure to list words that are very common among responses\n",
    "# For example, if the topics involve scientific research, include words like 'researchers' or 'study' and 'studies'. \n",
    "# Check which keywords result from the algorithm, and if there are many repeats that might clutter an image search, \n",
    "# be sure to include them below\n",
    "disallowed_words = ['question', 'phrase', 'questions', 'phrases', 'sentence', 'sentences', 'article', 'articles',\n",
    "                              'paragraph', 'paragraphs', 'why', 'how', 'did', 'what', 'who', 'where', 'when', 'do',\n",
    "                              'researchers', 'study']\n",
    "\n",
    "# initialize var\n",
    "final_search_words = []\n",
    "\n",
    "# function for filtering everything but nouns\n",
    "is_noun = lambda pos: (pos[:2] == 'NN' or pos[:2] == 'NNS' or pos[:2] == 'NNP' or pos[:2] == 'NNSP')\n",
    "\n",
    "# get keyword\n",
    "for i, x in enumerate(text_results):\n",
    "    # exclude eveything except nouns (including plural and proper); remove disallowed words\n",
    "    nltk_search_words[i] = [word for (word, pos) in nltk.pos_tag(nltk_search_words[i]) if is_noun(pos) and word not in disallowed_words]\n",
    "    pos_search_words[i] = [word for (word, pos) in nltk.pos_tag(pos_search_words[i]) if is_noun(pos) and word not in disallowed_words]  \n",
    "    \n",
    "    \n",
    "    # words that show up the most frequently and more than once in the nltk nouns get priority\n",
    "\n",
    "    # sort on basis of frequency of elements\n",
    "    temp_nltk_s_w = [item for items, c in Counter(nltk_search_words[i]).most_common() for item in [items] * c]\n",
    "    # remove unique values\n",
    "    for index in range(len(temp_nltk_s_w) - 1, -1, -1):\n",
    "        if temp_nltk_s_w.count(temp_nltk_s_w[index]) == 1:\n",
    "            del temp_nltk_s_w[index]\n",
    "    # temp_nltk_s_w now contains only words that occur more than once in nltk_search_words, sorted \n",
    "    # by those with the most duplicates first.\n",
    "    # temp_nltk_s_w does not contain any unique words. For example: ['word1', 'word1', 'word1', 'word2', word2']\n",
    "    # now, remove duplicates... \n",
    "    temp_nltk_s_w = list(dict.fromkeys(temp_nltk_s_w))\n",
    "    # ...and append this list to the final_search_words (first truncate list if there are too many)\n",
    "    for i in range(0, len(temp_nltk_s_w) - num_search_words):\n",
    "        temp_nltk_s_w.pop()\n",
    "    final_search_words.append(temp_nltk_s_w)\n",
    "    \n",
    "    \n",
    "    # if the list of search words is not completely filled, use classification to find which words in the sentence have the best fit\n",
    "    if len(final_search_words[i]) < num_search_words:\n",
    "        cl_res = classifier(x, pos_search_words[i], multi_label=True)['labels']\n",
    "        fkw_index = 0\n",
    "        while len(final_search_words[i]) < num_search_words and fkw_index < len(pos_search_words[i]):\n",
    "            while cl_res[fkw_index] in temp_nltk_s_w:\n",
    "                fkw_index += 1\n",
    "                if fkw_index >= len(pos_search_words[i]):\n",
    "                    break\n",
    "            final_search_words[i].append(cl_res[fkw_index])\n",
    "        # for reference: classifier(sequence, candidate_labels, multi_label=True)\n",
    "    #print(\"iteration\", i, \":\", final_search_words)\n",
    "    \n",
    "print(final_search_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pexels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the higher the photos_per_request, the more variation there will be in images, but the less relevance the images might have\n",
    "photos_per_request = 2\n",
    "image_size = 'large'  # 'original', 'large2x', 'large', 'medium', 'small', 'portrait', 'landscape', or 'tiny'\n",
    "\n",
    "API_KEY = '563492ad6f91700001000001c12998557b2d40488b7bc8d328bccd80'\n",
    "# The API key is relatively easy to get by creating an account at https://www.pexels.com/api/\n",
    "# It offers 200 requests per hour and 20,000 requests per month for free. \n",
    "\n",
    "default_query = 'science'\n",
    "# NOTE: The default query is here in case no results are found in the image search with the keyword(s) found above.\n",
    "# It may work well to enter a single word that is similar to the topics of other posts/images.\n",
    "# If there is no such theme, a good default may be 'nature' or 'food' or 'people', for example. \n",
    "# Any post that requires this 'placeholder' image will have 'PLACEHOLDER' at the end of its filename.\n",
    "\n",
    "for i, x in enumerate(final_search_words):\n",
    "    image_found = 1\n",
    "    # construct search query\n",
    "    my_query = final_search_words[i][0]\n",
    "    for j in range(1, len(final_search_words[i])):\n",
    "        my_query += \"%20\" + final_search_words[i][j]\n",
    "    print(text_results_punct[i])\n",
    "    response = requests.get(f\"https://api.pexels.com/v1/search?query=\"+my_query+\"&per_page=\"+str(photos_per_request)+\", allow_redirects=True\", headers={'Authorization': API_KEY})\n",
    "    if len(response.json()['photos']) == 0: # if nothing found in original query, fill it in with a picture of nature\n",
    "        response = requests.get(f\"https://api.pexels.com/v1/search?query=\"+default_query+\"&per_page=\"+str(photos_per_request)+\", allow_redirects=True\", headers={'Authorization': API_KEY})\n",
    "        image_found = 0\n",
    "    this_image = random.choice(response.json()['photos'])\n",
    "    print(this_image['url'])\n",
    "    # download image\n",
    "    image_data = requests.get(str(this_image['src'][str(image_size)])).content\n",
    "    image_name = 'images/image_' + str(i) + '.png'\n",
    "    if not image_found: image_name = 'images/image_' + str(i) + '_PLACEHOLDER' + '.png'\n",
    "    with open(image_name, 'wb') as handler:\n",
    "        handler.write(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other image APIs (not in use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsplash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download an image off unsplash without the api using python\n",
    "# https://www.codegrepper.com/code-examples/python/download+unsplash+images+python+without+api\n",
    "\n",
    "# assume no photos to begin with inside 'images' folder\n",
    "\n",
    "def downloadimage(search_term, num_imgs): # Define the function to download images\n",
    "    print(f\"https://source.unsplash.com/random/?\"+str(search_term)+\", allow_redirects=True\") # State the URL                                                                                      # Loop for chosen amount of times\n",
    "    num_imgs += 1\n",
    "    response = requests.get(f\"https://source.unsplash.com/random/?\"+str(search_term)+\", allow_redirects=True\")     # Download the photo(s)\n",
    "    print(\"Saving to: images/image\" + \"_\" + str(num_imgs) + \".png\")                                                # State the filename\n",
    "    open(\"images/image\" + \"_\" + str(num_imgs) + \".png\", 'wb').write(response.content)                              # Write image file\n",
    "\n",
    "\n",
    "# get all pictures\n",
    "for i, x in enumerate(final_search_words):\n",
    "    downloadimage(x, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"https://source.unsplash.com/search/photos?query=office, allow_redirects=True\")\n",
    "open(\"images/image_test.png\", 'wb').write(response.content)\n",
    "if response.headers['X-Imgix-ID'] == '104702ca07cd7ae5eeae32a67f307203f17a8128': # X-Imgix-ID for when image is not found\n",
    "    print('Image not found')\n",
    "else:\n",
    "    print('Image found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://api.unsplash.com/search/photos?page=1&query=office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"https://source.unsplash.com/random?women&order_by=relevant, allow_redirects=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikimedia Commons - consider a google image search api instead for a more flexible image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request\n",
    "\n",
    "url = \"https://commons.wikimedia.org/w/api.php?action=query&generator=images&prop=imageinfo&gimlimit=500&redirects=1&titles=Cat&iiprop=timestamp|user|userid|comment|canonicaltitle|url|size|dimensions|sha1|mime|thumbmime|mediatype|bitdepth\"\n",
    "\n",
    "params = {'format':'json'}\n",
    "\n",
    "res_img = requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'format':'json'}\n",
    "image_results = []\n",
    "    \n",
    "for i, x in enumerate(text_results):\n",
    "    url_query = ''\n",
    "    for j, x in enumerate(search_words[i]):\n",
    "        if j < 1: # ONLY the first two keywords\n",
    "            if j != 0:\n",
    "                url_query += '|'\n",
    "            url_query += search_words[i][j]\n",
    "    url_query = 'food' # food works, foods doesn't\n",
    "    print(url_query)\n",
    "    url = \"https://commons.wikimedia.org/w/api.php?action=query&generator=images&prop=imageinfo&gimlimit=500&redirects=1&titles=\" + url_query + \"&iiprop=url\"\n",
    "    res_img = requests.get(url, params)\n",
    "    print(res_img.status_code)\n",
    "    #print(res_img.json())\n",
    "    res_img_keys = list(res_img.json()['query']['pages'])\n",
    "    #### TODO: randomize the res_img_keys index so that different pictures show up for the same keyword\n",
    "    image_results.append(res_img.json()['query']['pages'][res_img_keys[0]]['imageinfo'][0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import os\n",
    "\n",
    "# get images\n",
    "google_Crawler = GoogleImageCrawler(storage = {'root_dir': r'images'})\n",
    "filters = dict(size='large', license='commercial,modify')\n",
    "google_Crawler.crawl(keyword = 'positive phrase', filters=filters, max_num = 5)\n",
    "# https://icrawler.readthedocs.io/en/latest/builtin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('images/000001.png','images/100001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible google image search api\n",
    "\n",
    "from urllib.request import build_opener\n",
    "import simplejson\n",
    "from io import StringIO\n",
    "\n",
    "fetcher = urllib.build_opener()\n",
    "searchTerm = 'parrot'\n",
    "startIndex = 0\n",
    "searchUrl = \"http://ajax.googleapis.com/ajax/services/search/images?v=1.0&q=\" + searchTerm + \"&start=\" + startIndex\n",
    "f = fetcher.open(searchUrl)\n",
    "deserialized_output = simplejson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
